{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f704046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import permutations\n",
    "from scipy.special import kl_div\n",
    "import itertools\n",
    "import numpy as np\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3202e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(clip.available_models())\n",
    "clip_model, preprocess = clip.load('ViT-B/32', device)\n",
    "clip_model = clip_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12aa1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_path = \"/home/samyakr2/multilabel/ARK/pascal_train_clip_features_vit14.pt\"\n",
    "train_labels_path = '/home/samyakr2/multilabel/ARK/pascal_train_clip_labels_vit14.pt'\n",
    "val_features_path = \"/home/samyakr2/multilabel/ARK/pascal_val_clip_features_vit14.pt\"\n",
    "val_labels_path = '/home/samyakr2/multilabel/ARK/pascal_val_clip_labels_vit14.pt'\n",
    "\n",
    "train_features = torch.load(train_features_path)\n",
    "train_labels = torch.load(train_labels_path)\n",
    "val_features = torch.load(val_features_path)\n",
    "val_labels = torch.load(val_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1660ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class clip_2fc(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(clip_2fc, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim),\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe699f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1721681/3134605782.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 50.16105733811855\n",
      "Epoch [2/10], Loss: 21.40542382746935\n",
      "Epoch [3/10], Loss: 18.183908856473863\n",
      "Epoch [4/10], Loss: 16.408542413264513\n",
      "Epoch [5/10], Loss: 15.083921588025987\n",
      "Epoch [6/10], Loss: 13.954736817860976\n",
      "Epoch [7/10], Loss: 12.96553359576501\n",
      "Epoch [8/10], Loss: 12.03550035203807\n",
      "Epoch [9/10], Loss: 11.158271201653406\n",
      "Epoch [10/10], Loss: 10.342951761791483\n"
     ]
    }
   ],
   "source": [
    "input_size = train_features[0].size(1)  \n",
    "hidden_size = 100  # Define the size of the hidden layer\n",
    "num_classes = len(train_labels[0][0])  # Assuming labels_batches is a list of lists of labels\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "print(input_size)\n",
    "# Initialize the model\n",
    "model = clip_2fc(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_model_state_dict = None\n",
    "best_loss = float('inf')\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for features_batch, labels_batch in zip(train_features, train_labels):\n",
    "        # Flatten features batch\n",
    "        features_batch = features_batch.view(features_batch.size(0), -1)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)\n",
    "        # Forward pass\n",
    "        outputs = model(features_batch.to(device))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels_tensor.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print loss for the epoch\n",
    "#     if epoch %100 == 0:\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_model_state_dict = model.state_dict()\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model_state_dict, \"/home/samyakr2/multilabel/ARK/clip+2fc/best_clip_2fc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4dfe906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.056872614582611634\n",
      "Average Precision Score: 0.9326465316566379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1721681/2659371570.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Define a function for testing the model\n",
    "def test_model(model, criterion, features_batches, labels_batches, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for features_batch, labels_batch in zip(features_batches, labels_batches):\n",
    "            # Move batch to device\n",
    "            features_batch = features_batch.to(device)\n",
    "            labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Flatten features batch\n",
    "            features_batch = features_batch.view(features_batch.size(0), -1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels_tensor)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert outputs and labels to numpy arrays\n",
    "            outputs_np = outputs.cpu().detach().numpy()\n",
    "            labels_np = labels_tensor.cpu().detach().numpy()\n",
    "\n",
    "            all_outputs.append(outputs_np)\n",
    "            all_labels.append(labels_np)\n",
    "\n",
    "    # Concatenate outputs and labels\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Compute average precision score\n",
    "    avg_precision = average_precision_score(all_labels, all_outputs, average='micro')\n",
    "\n",
    "    # Average test loss\n",
    "    avg_test_loss = test_loss / len(features_batches)\n",
    "    print(f\"Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Average Precision Score: {avg_precision}\")\n",
    "\n",
    "\n",
    "best_model_state_dict = torch.load(\"/home/samyakr2/multilabel/ARK/clip+2fc/best_clip_2fc.pth\")\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "test_model(model, criterion, val_features, val_labels, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389a2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
