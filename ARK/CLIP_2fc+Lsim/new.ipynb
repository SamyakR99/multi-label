{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f219bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f915563b290>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import permutations\n",
    "from scipy.special import kl_div\n",
    "import itertools\n",
    "import numpy as np\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6e1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# # print(clip.available_models())\n",
    "# clip_model, preprocess = clip.load('ViT-L/14', device)\n",
    "# clip_model = clip_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959dba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_pascal = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "# items_pascal = [\"A photo of a \" + item for item in labels_pascal]\n",
    "\n",
    "# text = clip.tokenize(items_pascal).to(device)\n",
    "# text_features = clip_model.encode_text(text)\n",
    "# text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# text_features_path = '/home/samyakr2/multilabel/ARK/new_idea/pascal_labels_features.pt'\n",
    "# torch.save(text_features, text_features_path)\n",
    "# ### For code to obtain .npy file below look at ll_sim.py in github\n",
    "# # y = np.load('/home/samyakr2/multilabel/data/pascal/pascal_ll_sim_vit14.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69c7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class projector(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(projector, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim),\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "#         out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class projector2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(projector2, self).__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, input_dim, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(input_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = 0.15*x + 0.85*out\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# def Projector(args, embedding):\n",
    "#     mlp_spec = f\"{embedding}-{args.mlp}\"\n",
    "#     layers = []\n",
    "#     f = list(map(int, mlp_spec.split(\"-\")))\n",
    "#     for i in range(len(f) - 2):\n",
    "#         layers.append(nn.Linear(f[i], f[i + 1]))\n",
    "#         layers.append(nn.BatchNorm1d(f[i + 1]))\n",
    "#         layers.append(nn.ReLU(True))\n",
    "#     layers.append(nn.Linear(f[-2], f[-1], bias=False))\n",
    "#     return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce6f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "text_features_path = '/home/samyakr2/multilabel/ARK/new_idea/pascal_labels_features.pt'\n",
    "text_features = torch.load(text_features_path)\n",
    "\n",
    "# in_s = text_features.shape[1]  \n",
    "# hs =text_features.shape[1]    # Define the size of the hidden layer\n",
    "# nc = text_features.shape[1]  \n",
    "\n",
    "# model_text = projector(in_s, hs, nc).to(device)\n",
    "# model_text2 = projector2(input_size, hidden_size, num_classes).to(device)\n",
    "# outputs_text = model_text(text_features.to(device))\n",
    "# # outputs_text2 = model_text2(text_features.to(device))\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# similarity_text = (outputs_text @ outputs_text.T) \n",
    "# # similarity_text2 = (outputs_text2 @ outputs_text2.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa842dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(clip.available_models())\n",
    "clip_model, preprocess = clip.load('RN50x64', device)\n",
    "clip_model = clip_model.float()\n",
    "\n",
    "train_features_path = \"/home/samyakr2/multilabel/ARK/pascal_train_clip_features_vit14.pt\"\n",
    "train_labels_path = '/home/samyakr2/multilabel/ARK/pascal_train_clip_labels_vit14.pt'\n",
    "val_features_path = \"/home/samyakr2/multilabel/ARK/pascal_val_clip_features_vit14.pt\"\n",
    "val_labels_path = '/home/samyakr2/multilabel/ARK/pascal_val_clip_labels_vit14.pt'\n",
    "\n",
    "train_features = torch.load(train_features_path)\n",
    "train_labels = torch.load(train_labels_path)\n",
    "val_features = torch.load(val_features_path)\n",
    "val_labels = torch.load(val_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27b4620e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1725808/615086401.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)#.clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/42], Loss: 96.85115922987461\n",
      "Epoch [2/42], Loss: 78.61294142901897\n",
      "Epoch [3/42], Loss: 68.86607925593853\n",
      "Epoch [4/42], Loss: 62.32768606394529\n",
      "Epoch [5/42], Loss: 58.787944830954075\n",
      "Epoch [6/42], Loss: 54.73753057420254\n",
      "Epoch [7/42], Loss: 46.13943652436137\n",
      "Epoch [8/42], Loss: 40.78174713626504\n",
      "Epoch [9/42], Loss: 37.012149553745985\n",
      "Epoch [10/42], Loss: 34.261501701548696\n",
      "Epoch [11/42], Loss: 32.05996234156191\n",
      "Epoch [12/42], Loss: 30.579210763797164\n",
      "Epoch [13/42], Loss: 29.379332654178143\n",
      "Epoch [14/42], Loss: 28.411359798163176\n",
      "Epoch [15/42], Loss: 27.47197165340185\n",
      "Epoch [16/42], Loss: 26.538577096536756\n",
      "Epoch [17/42], Loss: 25.591291468590498\n",
      "Epoch [18/42], Loss: 24.749044815078378\n",
      "Epoch [19/42], Loss: 23.911655995063484\n",
      "Epoch [20/42], Loss: 23.176474997773767\n",
      "Epoch [21/42], Loss: 22.480670656077564\n",
      "Epoch [22/42], Loss: 21.890956847928464\n",
      "Epoch [23/42], Loss: 21.338389019481838\n",
      "Epoch [24/42], Loss: 20.775641130283475\n",
      "Epoch [25/42], Loss: 20.283374114893377\n",
      "Epoch [26/42], Loss: 19.845710144843906\n",
      "Epoch [27/42], Loss: 19.40721056982875\n",
      "Epoch [28/42], Loss: 19.029145072679967\n",
      "Epoch [29/42], Loss: 18.649438583292067\n",
      "Epoch [30/42], Loss: 18.247349789831787\n",
      "Epoch [31/42], Loss: 17.875253250356764\n",
      "Epoch [32/42], Loss: 17.498139909934253\n",
      "Epoch [33/42], Loss: 17.131538952235132\n",
      "Epoch [34/42], Loss: 16.776943129952997\n",
      "Epoch [35/42], Loss: 16.436441007070243\n",
      "Epoch [36/42], Loss: 16.12800097092986\n",
      "Epoch [37/42], Loss: 15.801091065164655\n",
      "Epoch [38/42], Loss: 15.479463158873841\n",
      "Epoch [39/42], Loss: 15.162336437730119\n",
      "Epoch [40/42], Loss: 14.848323367536068\n",
      "Epoch [41/42], Loss: 14.51522473548539\n",
      "Epoch [42/42], Loss: 14.191346500767395\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "class clip_2fc(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(clip_2fc, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        )\n",
    "        \n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim),\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "#         out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "input_size = train_features[0].size(1)  \n",
    "hidden_size = 100  # Define the size of the hidden layer\n",
    "num_classes = len(train_labels[0][0])  # Assuming labels_batches is a list of lists of labels\n",
    "\n",
    "in_s = text_features.shape[1]  \n",
    "hs = 200  # Define the size of the hidden layer\n",
    "nc = 100  \n",
    "\n",
    "print(in_s)\n",
    "# Initialize the model\n",
    "model = clip_2fc(input_size, hidden_size, num_classes).to(device)\n",
    "model_text = projector(in_s, hs, nc).to(device)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for multilabel classification\n",
    "\n",
    "params_to_optimize = list(model.parameters()) + list(model_text.parameters())\n",
    "optimizer = torch.optim.Adam(params_to_optimize, lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# # Training loop\n",
    "\n",
    "best_loss = float('inf')\n",
    "num_epochs = 42\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for features_batch, labels_batch in zip(train_features, train_labels):\n",
    "        # Flatten features batch\n",
    "        features_batch = features_batch.view(features_batch.size(0), -1)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels_tensor = torch.tensor(labels_batch, dtype=torch.float32)#.clone().detach()\n",
    "        # Forward pass\n",
    "        outputs = model(features_batch.to(device))\n",
    "        outputs_reshaped = outputs.unsqueeze(-1)\n",
    "#         outputs_reshaped_normalized = F.normalize(outputs_reshaped, p=2, dim=1)\n",
    "        \n",
    "        \n",
    "        outputs_text = model_text(text_features.to(device))\n",
    "        similarity_text = (outputs_text @ outputs_text.T)\n",
    "#         print(similarity_text.unsqueeze(0).expand(outputs.shape[0], -1, -1).shape)\n",
    "#         print(outputs_reshaped.shape)\n",
    "        normalized_similarity_text = F.normalize(similarity_text, p=2, dim=1)  # Normalize along the second dimension (rows)\n",
    "        normalized_similarity_text = torch.clamp(normalized_similarity_text, min=0, max=1)  # Clamp values to be between 0 and 1\n",
    "\n",
    "        result = outputs_reshaped * normalized_similarity_text.unsqueeze(0).expand(outputs.shape[0], -1, -1)\n",
    "\n",
    "#         result = outputs_reshaped * similarity_text.unsqueeze(0).expand(outputs.shape[0], -1, -1)\n",
    "        \n",
    "        pred = result.sum(dim=1) / 16\n",
    "#         print(\"pred shape\", pred.shape)\n",
    "        pred = torch.sigmoid(pred)\n",
    "#         Compute loss\n",
    "        loss = criterion(pred, labels_tensor.to(device))\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "#         print(\"==\"*50)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}\")\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_text_model_dict = model_text.state_dict()\n",
    "    \n",
    "    # Save the model every 20 epochs\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        torch.save(best_model_state_dict, f\"/home/samyakr2/multilabel/ARK/new_idea/best_epoch_{epoch+1}.pth\")\n",
    "        torch.save(best_text_model_dict, f\"/home/samyakr2/multilabel/ARK/new_idea/best_text_epoch_{epoch+1}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab53e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08354618316413936\n",
      "Average Precision Score: 0.87098610162858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1725808/3583956179.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n",
      "/tmp/ipykernel_1725808/3583956179.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.06926569649130924\n",
      "Average Precision Score: 0.9110193713462178\n",
      "Test Loss: 0.6703532842489389\n",
      "Average Precision Score: 0.21433031945027187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1725808/3583956179.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/samyakr2/multilabel/ARK/new_idea/best_epoch_80.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Precision Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 58\u001b[0m     best_model_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/samyakr2/multilabel/ARK/new_idea/best_epoch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n\u001b[1;32m     59\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model_state_dict)\n\u001b[1;32m     61\u001b[0m     best_model_state_dict_text \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/samyakr2/multilabel/ARK/new_idea/best_text_epoch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/samyakr2/multilabel/ARK/new_idea/best_epoch_80.pth'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Define a function for testing the model\n",
    "def test_model(model, model_text,text_features,criterion, features_batches, labels_batches, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for features_batch, labels_batch in zip(features_batches, labels_batches):\n",
    "            # Move batch to device\n",
    "            features_batch = features_batch.to(device)\n",
    "            labels_tensor = torch.tensor(labels_batch, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Flatten features batch\n",
    "            features_batch = features_batch.view(features_batch.size(0), -1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features_batch)\n",
    "            outputs_text = model_text(text_features.to(device))\n",
    "#             similarity_text = (outputs_text @ outputs_text.T)\n",
    "#             print(outputs)\n",
    "            outputs_reshaped = outputs.unsqueeze(-1)\n",
    "            \n",
    "            \n",
    "            result = outputs_reshaped * normalized_similarity_text.unsqueeze(0).expand(outputs.shape[0], -1, -1)\n",
    "        \n",
    "            pred = result.sum(dim=1) / 16\n",
    "    #         print(\"pred shape\", pred.shape)\n",
    "#             pred = torch.sigmoid(pred)\n",
    "#             loss = criterion(pred, labels_tensor)\n",
    "\n",
    "            loss = criterion(torch.sigmoid(pred), labels_tensor)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Convert outputs and labels to numpy arrays\n",
    "            outputs_np = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            labels_np = labels_tensor.cpu().detach().numpy()\n",
    "\n",
    "            all_outputs.append(outputs_np)\n",
    "            all_labels.append(labels_np)\n",
    "\n",
    "    # Concatenate outputs and labels\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Compute average precision score\n",
    "    avg_precision = average_precision_score(all_labels, all_outputs, average='micro')\n",
    "\n",
    "    # Average test loss\n",
    "    avg_test_loss = test_loss / len(features_batches)\n",
    "    print(f\"Test Loss: {avg_test_loss}\")\n",
    "    print(f\"Average Precision Score: {avg_precision}\")\n",
    "\n",
    "for i in range (20,200,20):\n",
    "    best_model_state_dict = torch.load(\"/home/samyakr2/multilabel/ARK/new_idea/best_epoch_{}.pth\".format(i))\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    best_model_state_dict_text = torch.load(\"/home/samyakr2/multilabel/ARK/new_idea/best_text_epoch_{}.pth\".format(i))\n",
    "    model_text.load_state_dict(best_model_state_dict_text)\n",
    "\n",
    "    test_model(model, model_text, text_features, criterion, val_features, val_labels, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f2a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
